{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4227ce37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DistilSR import *\n",
    "exp_set = get_exp_set(3)\n",
    "print(len(exp_set))\n",
    "exp_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f4c091",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pmlb import fetch_data, regression_dataset_names\n",
    "import time\n",
    "import re\n",
    "import multiprocessing\n",
    "from collections.abc import Iterable\n",
    "\n",
    "def cost(x, xdata, ydata, lambda_string): \n",
    "    y_pred = eval(lambda_string)(x, xdata)\n",
    "    return np.mean(((y_pred - ydata))**2) \n",
    "\n",
    "def cost_special(x, xdata, ydata, lambda_string,T): \n",
    "    y_pred = eval(lambda_string)(x, xdata)\n",
    "    loss_list = (y_pred - ydata)**2\n",
    "    return (np.mean(np.abs(T-loss_list)), sum(T>=loss_list)/len(ydata) )\n",
    "\n",
    "def cust_pred(x, xdata, ydata, lambda_string): \n",
    "    y_pred = eval(lambda_string)(x, xdata)\n",
    "    return y_pred\n",
    "\n",
    "def save_strings_to_file(strings, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        for string in strings:\n",
    "            file.write(string + '\\n')\n",
    "\n",
    "def load_strings_from_file(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            yield line.strip()\n",
    "    \n",
    "total_time = 0\n",
    "\n",
    "def process_eq(chunk_idx, dataset_name, train_X, train_y, test_X, test_y,random_state):\n",
    "    results = []\n",
    "    total_time_chunk = 0\n",
    "    eq_list_chunk = load_strings_from_file(f'strings_data_TEMP_chunk_{chunk_idx}.txt')\n",
    "    for test_eq in eq_list_chunk:\n",
    "        ERC_count = test_eq.count(\"x[\")\n",
    "        lambda_string = \"lambda x,xdata:\" + test_eq\n",
    "        if ERC_count:\n",
    "            try:\n",
    "                np.random.seed(random_state)\n",
    "                start_time = time.time()\n",
    "                res = minimize(cost,\n",
    "                               x0=np.random.randint(1, 31, size=ERC_count)/10,\n",
    "                               args=(train_X.T, train_y, lambda_string),\n",
    "                               method=\"BFGS\", options={\"maxiter\": 500})\n",
    "                total_time_chunk+=time.time() - start_time\n",
    "                optimized_cost = cost(res.x, train_X.T, train_y, lambda_string) #train loss\n",
    "                results.append((lambda_string, res.x, res.nit, optimized_cost))\n",
    "            except RuntimeError:\n",
    "                results.append((lambda_string, None, None, None))\n",
    "        else:\n",
    "            optimized_cost = cost(None, train_X.T, train_y, lambda_string)\n",
    "            results.append((lambda_string, None, None, optimized_cost))\n",
    "    return results, total_time_chunk\n",
    "\n",
    "def process_eq_one_out(chunk_idx, dataset_name, train_X, train_y, test_X, test_y,random_state):\n",
    "    results = []\n",
    "    total_time_chunk = 0\n",
    "    eq_list_chunk = load_strings_from_file(f'strings_data_TEMP_chunk_{chunk_idx}.txt')\n",
    "    for test_eq in eq_list_chunk:\n",
    "        ERC_count = test_eq.count(\"x[\")\n",
    "        lambda_string = \"lambda x,xdata:\" + test_eq\n",
    "        if ERC_count:\n",
    "            try:\n",
    "                np.random.seed(random_state)\n",
    "                start_time = time.time()\n",
    "                res = minimize(cost,\n",
    "                               x0=np.random.randint(1, 31, size=ERC_count)/10,\n",
    "                               args=(train_X[:-1].T, train_y[:-1], lambda_string),\n",
    "                               method=\"BFGS\", options={\"maxiter\": 500})\n",
    "                total_time_chunk+=time.time() - start_time\n",
    "                optimized_cost = cost(res.x, train_X[:-1].T, train_y[:-1], lambda_string)\n",
    "                results.append((lambda_string, res.x, res.nit, optimized_cost))\n",
    "            except RuntimeError:\n",
    "                results.append((lambda_string, None, None, None))\n",
    "        else:\n",
    "            optimized_cost = cost(None, train_X[:-1].T, train_y[:-1], lambda_string)\n",
    "            results.append((lambda_string, None, None, optimized_cost))\n",
    "    return results, total_time_chunk\n",
    "\n",
    "def test_SR(dataset_name,random_state=0):\n",
    "    df = pd.read_csv(\"SRSDdatasets/\"+dataset_name+\".txt\", sep=' ', header=None)\n",
    "    total_time = 0\n",
    "    X, y = np.array(df.iloc[:,:-1]), np.array(df.iloc[:,-1])\n",
    "    train_X, test_X, train_y, test_y = train_test_split(X, y, random_state=random_state,test_size=(8000-20)/8000)\n",
    "    np.random.seed(random_state)\n",
    "    shuffle_idx = np.random.permutation(len(train_y))\n",
    "    train_X = train_X[shuffle_idx]\n",
    "    train_y = train_y[shuffle_idx]\n",
    "    master_list=[]\n",
    "    eq_list = []\n",
    "    num_of_feature = train_X.shape[1]\n",
    "    mse_tuple = tuple()\n",
    "    for test_eq in tqdm(exp_set):\n",
    "        test_eq_orig = test_eq\n",
    "        R_count = test_eq.count(\"R\")\n",
    "        \n",
    "        for combi_var in itertools.product(range(num_of_feature+1), repeat=R_count):\n",
    "            test_eq=test_eq_orig\n",
    "            for i in combi_var:\n",
    "                if i==num_of_feature:\n",
    "                    test_eq = test_eq.replace(\"R\", \"erc\", 1)\n",
    "                else:\n",
    "                    test_eq = test_eq.replace(\"R\", f\"xdata[{i}]\", 1)\n",
    "            match = re.search(r\"\\w{3}\\(erc,erc\\)\", test_eq)\n",
    "            while match:\n",
    "                test_eq = test_eq.replace(match.group(),\"erc\")\n",
    "                match = re.search(r\"\\w{3}\\(erc,erc\\)\", test_eq)\n",
    "            ERC_count = test_eq.count(\"erc\")\n",
    "            for i in range(ERC_count):\n",
    "                test_eq = test_eq.replace(\"erc\", f\"x[{i}]\", 1)\n",
    "            eq_list.append(test_eq)\n",
    "    eq_list = list(set(eq_list))\n",
    "    num_processes = multiprocessing.cpu_count()-1\n",
    "    eq_list_len = len (eq_list)\n",
    "    chunk_size = eq_list_len // num_processes\n",
    "    for idx, i in enumerate(range(0, eq_list_len, chunk_size)):\n",
    "        save_strings_to_file(eq_list[i:i + chunk_size], f'strings_data_TEMP_chunk_{idx}.txt')\n",
    "    del eq_list\n",
    "\n",
    "    with multiprocessing.Pool(processes=num_processes) as pool:\n",
    "        results = pool.starmap(process_eq, [(chunk_idx, dataset_name, train_X, train_y, test_X, test_y,random_state) for chunk_idx in range(num_processes)])\n",
    "\n",
    "    for result_chunk, total_time_chunk in results:\n",
    "        master_list.extend(result_chunk)\n",
    "        total_time+=total_time_chunk\n",
    "    print(total_time)\n",
    "\n",
    "    pd.DataFrame(master_list).to_csv(\"TEMP_checker1.csv\")\n",
    "    temp_holder = min(master_list,key = lambda x: x[3])\n",
    "    print(f\"{temp_holder=}\")\n",
    "    \n",
    "    master_list=[]\n",
    "    eq_list = []\n",
    "    num_of_feature = train_X.shape[1]\n",
    "    mse_tuple = tuple()\n",
    "    for test_eq in tqdm(exp_set):\n",
    "        test_eq_orig = test_eq\n",
    "        R_count = test_eq.count(\"R\")\n",
    "        \n",
    "        for combi_var in itertools.product(range(num_of_feature+1), repeat=R_count):\n",
    "            test_eq=test_eq_orig\n",
    "            for i in combi_var:\n",
    "                if i==num_of_feature:\n",
    "                    test_eq = test_eq.replace(\"R\", \"erc\", 1)\n",
    "                else:\n",
    "                    test_eq = test_eq.replace(\"R\", f\"xdata[{i}]\", 1)\n",
    "            match = re.search(r\"\\w{3}\\(erc,erc\\)\", test_eq)\n",
    "            while match:\n",
    "                test_eq = test_eq.replace(match.group(),\"erc\")\n",
    "                match = re.search(r\"\\w{3}\\(erc,erc\\)\", test_eq)\n",
    "            ERC_count = test_eq.count(\"erc\")\n",
    "            for i in range(ERC_count):\n",
    "                test_eq = test_eq.replace(\"erc\", f\"x[{i}]\", 1)\n",
    "            eq_list.append(test_eq)\n",
    "    eq_list = list(set(eq_list))\n",
    "    num_processes = multiprocessing.cpu_count()*2//3\n",
    "    eq_list_len = len (eq_list)\n",
    "    chunk_size = eq_list_len // num_processes\n",
    "    for idx, i in enumerate(range(0, eq_list_len, chunk_size)):\n",
    "        save_strings_to_file(eq_list[i:i + chunk_size], f'strings_data_TEMP_chunk_{idx}.txt')\n",
    "    del eq_list\n",
    "\n",
    "    with multiprocessing.Pool(processes=num_processes) as pool:\n",
    "        results = pool.starmap(process_eq_one_out, [(chunk_idx, dataset_name, train_X, train_y, test_X, test_y,random_state) for chunk_idx in range(num_processes)])\n",
    "\n",
    "    for result_chunk, total_time_chunk in results:\n",
    "        master_list.extend(result_chunk)\n",
    "        total_time+=total_time_chunk\n",
    "    print(total_time)\n",
    "\n",
    "    pd.DataFrame(master_list).to_csv(\"TEMP_checker2.csv\")\n",
    "    \n",
    "    temp_holder2 = min(master_list,key = lambda x: x[3])\n",
    "    print(f\"{temp_holder2=}\")\n",
    "\n",
    "    temp_holder_main = (cost(temp_holder[1], test_X.T, test_y, temp_holder[0]),\n",
    "                        cost(temp_holder[1], train_X[[-1]].T, train_y[[-1]], temp_holder[0]), #V(f_{SUz},z)\n",
    "                       )\n",
    "    temp_holder_main2 = (temp_holder2[3], #train loss\n",
    "                          cost(temp_holder2[1], test_X.T, test_y, temp_holder2[0]), #test loss\n",
    "                          cost(temp_holder2[1], train_X[[-1]].T, train_y[[-1]], temp_holder2[0]), #V(f_S,z)\n",
    "                         ) + cost_special(temp_holder2[1], train_X[:-1].T, train_y[:-1], temp_holder2[0], temp_holder_main[1]) #|V(f_{SUz},z)-V(f_S,z_i)|\n",
    "\n",
    "    y_std = np.std(test_y)\n",
    "    try:\n",
    "        test_r2 = r2_score(test_y,cust_pred(temp_holder2[1], test_X.T, test_y, temp_holder2[0])), #test r2\n",
    "    except:\n",
    "        test_r2 = None\n",
    "    temp_final = (np.abs(temp_holder_main2[1]-temp_holder_main2[0]), #Raw generalization gap\n",
    "                  np.abs(temp_holder_main2[2]-temp_holder_main[1]), #Raw APG\n",
    "                  temp_holder_main2[3], #Raw ATG\n",
    "                  np.abs(temp_holder_main2[1]-temp_holder_main2[0])/(y_std**2), #generalization gap normalized\n",
    "                  np.abs(temp_holder_main2[2]-temp_holder_main[1])/(y_std**2), #APG normalized\n",
    "                  temp_holder_main2[3]/(y_std**2), #ATG normalized\n",
    "                 )\n",
    "\n",
    "    return temp_holder+temp_holder2+temp_holder_main+temp_holder_main2, temp_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cacaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "num_runs = 10\n",
    "directory = 'SRSDdatasets'\n",
    "for idx,filename in enumerate(os.listdir(directory)):\n",
    "    f = os.path.join(directory, filename)\n",
    "    if os.path.isfile(f):\n",
    "        dataset_name = filename[:-4]\n",
    "    else:\n",
    "        continue\n",
    "    print(f\"{dataset_name=}\")\n",
    "    master_df1 = []\n",
    "    master_df2 = []\n",
    "    for i in range(num_runs):\n",
    "        DistilSR_solution = test_SR(dataset_name,random_state=i)\n",
    "        master_df1.append(list(DistilSR_solution[0]))\n",
    "        master_df2.append(list(DistilSR_solution[1])) \n",
    "    pd.DataFrame(master_df1).to_csv(f\"R_{dataset_name}_full_{num_runs}.csv\",index=False)\n",
    "    pd.DataFrame(master_df2).to_csv(f\"R_{dataset_name}_{num_runs}.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": "24",
    "lenType": "24",
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 274,
   "position": {
    "height": "39.9937px",
    "left": "1725.44px",
    "right": "20px",
    "top": "115px",
    "width": "555.169px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
